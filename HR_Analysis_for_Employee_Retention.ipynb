{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zuckmo/HR-analysis-for-Employee-Retention/blob/main/HR_Analysis_for_Employee_Retention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDyfRGeOho9X"
      },
      "source": [
        "PACE STAGES\n",
        "#PLAN\n",
        "- Understanding the business scenario and problem\n",
        "- Familiarize with HR dataset\n",
        "1. step1 . Import\n",
        "2. step2 Data Exploration (initial eda and data cleaning-missing value, redudandat, outliers)\n",
        "\n",
        "#ANALYZE\n",
        "- Data exploration (continue EDA)\n",
        "\n",
        "#CONSTRUCT\n",
        "\n",
        "#EXECUTE\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4pqNSeojobG"
      },
      "source": [
        "#PLAN\n",
        "the company initiates to improve satisfaction levels. They collected data from HR department. this project is to analyze the data collected by HR department and to buid a model that can predict whether the employee will leave the company or not.\n",
        "\n",
        "if this project managed to predict the possibility of employee leaving, it might be possible to indentify some factors that contribute to their leaving. this will be helpful for the company to reduce cost of hiring new employees, interviewing employees and can increase employee retention.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDLueeQ1hBOi"
      },
      "outputs": [],
      "source": [
        "#IMPORTS\n",
        "\n",
        "#for data manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "#for data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "#for display all the columns in dataframes\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "#for data modelling\n",
        "from xgboost import XGBClassifier\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost import plot_importance\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "#for metrics and helpful functions\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "#for saving model\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-4g9ccGnSXz",
        "outputId": "58d800fe-6971-4461-df4a-f30fe4574b75"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ce9ef443-7e40-4f90-bf4d-7a9a00b6c163\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ce9ef443-7e40-4f90-bf4d-7a9a00b6c163\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#load dataset\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LpRCS4YoED5"
      },
      "outputs": [],
      "source": [
        "\n",
        "df = pd.read_csv(\"HR_comma_sep.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWkfaYvwoUvI"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TBhaupzhovNq"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oDfKMjro5JU"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItodJjzapZuQ"
      },
      "source": [
        "RENAME COLUMNS\n",
        "As a data cleaning step, rename the columns as needed. Standardize the column names so that they are all in snake_case, correct any column names that are misspelled, and make column names more concise as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJYppbGJpFaO"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SM6FQESpkuZ"
      },
      "outputs": [],
      "source": [
        "df = df.rename(columns={'average_montly_hours':'average_monthly_hours', 'time_spend_company':'tenure', 'Work_accident':'work_accident', 'Department':'department'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RAXfl4_nqPY3"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gzy_JeoWqoiO"
      },
      "source": [
        "**CHECKING MISSING VALUES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNhrkfjXqj8h"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kaFGy38qvQx"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZokBzXpq1Nw"
      },
      "outputs": [],
      "source": [
        "c = (3008/14999)*100\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmmbovIQrdr3"
      },
      "source": [
        "this is 20% of duplicates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tgs6Z1_arE5E"
      },
      "outputs": [],
      "source": [
        "df[df.duplicated()].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeyjxVCwruXL"
      },
      "outputs": [],
      "source": [
        "#drop duplicates and save in new variable\n",
        "df1 = df.drop_duplicates(keep='first') #if there is no keep the data will be error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnBRjbpSsDCP"
      },
      "outputs": [],
      "source": [
        "df1.tail()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaOUSuzXuaQq"
      },
      "source": [
        "**check outlier**\n",
        "only tenure has outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQP32tfRsFVZ"
      },
      "outputs": [],
      "source": [
        "sns.boxplot(data=df1['tenure'])  # Replace 'df' with your DataFrame\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oIEiUdGxpr9"
      },
      "source": [
        "it's valuabe to know how many rows that contain tenure outliers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzluaoElwKER"
      },
      "outputs": [],
      "source": [
        "#count 75%\n",
        "percentile75 = df1['tenure'].quantile(0.75)\n",
        "percentile25 = df1['tenure'].quantile(0.25)\n",
        "\n",
        "#hitung iqr\n",
        "iqr = percentile75 - percentile25\n",
        "\n",
        "#count upper and lower limit\n",
        "upper_limit = percentile75 + 1.5 * iqr\n",
        "lower_limit =percentile25 - 1.5 * iqr\n",
        "\n",
        "outliers = df1[(df1['tenure']>upper_limit) | (df1[\"tenure\"] < lower_limit)]\n",
        "\n",
        "print('number of data that contain outlier value of tenure :', len(outliers), \"rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JE0yuQnJw9Bt"
      },
      "outputs": [],
      "source": [
        "df1.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqrRIlrmAUBT"
      },
      "outputs": [],
      "source": [
        "# Create a heatmap using seaborn\n",
        "sns.heatmap(df1.corr(), annot=True, cmap=\"YlGnBu\")\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgv5G8kgBBPl"
      },
      "source": [
        "#insight\n",
        "It appears that employees are leaving the company as a result of poor management. Leaving is tied to longer working hours, many projects, and generally lower satisfaction levels. It can be ungratifying to work long hours and not receive promotions or good evaluation scores. There's a sizeable group of employees at this company who are probably burned out. It also appears that if an employee has spent more than six years at the company, they tend not to leave.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsRmYvDF_zQV"
      },
      "outputs": [],
      "source": [
        "df_en = df1.copy()\n",
        "\n",
        "# Define the mapping of ordinal values to numerical values\n",
        "ordinal_mapping = {\n",
        "    'low': 1,\n",
        "    'medium': 2,\n",
        "    'high': 3}\n",
        "\n",
        "#encode the salar as ordinal numeric category\n",
        "df_en['salary']= df_en['salary'].map(ordinal_mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrpnfqrhC6vu"
      },
      "outputs": [],
      "source": [
        "df_en.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOdAfefqD-EV"
      },
      "outputs": [],
      "source": [
        "df_en = pd.get_dummies(df_en, columns=[\"department\"], prefix=['department'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ywQSv0kEiAd"
      },
      "outputs": [],
      "source": [
        "df_en.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ugCme-SME0tz"
      },
      "outputs": [],
      "source": [
        "# Create a stacked bart plot to visualize number of employees across department, comparing those who left with those who didn't\n",
        "# In the legend, 0 (purple color) represents employees who did not leave, 1 (red color) represents employees who left\n",
        "pd.crosstab(df1['department'], df1['left']).plot(kind ='bar',color='mr')\n",
        "plt.title('Counts of employees who left versus stayed across department')\n",
        "plt.ylabel('Employee count')\n",
        "plt.xlabel('Department')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_Xv0KL9Fr-P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71GPO4FnF-2P"
      },
      "source": [
        "since logistric regression is sensitive to outliers, so remove the outliers in tenure column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE1gEQwtGG6P"
      },
      "outputs": [],
      "source": [
        "df_logreg = df_en[(df_en['tenure'] >= lower_limit) & (df_en['tenure'] <= upper_limit)]\n",
        "df_logreg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaKCGVi4GnFp"
      },
      "source": [
        "#isolate outcome variable(\"left\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21arJzNAGeKh"
      },
      "outputs": [],
      "source": [
        "y = df_logreg['left']\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM7o1N4HGzd1"
      },
      "outputs": [],
      "source": [
        "X = df_logreg.drop('left', axis=1)\n",
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prjl3TuKHICu"
      },
      "outputs": [],
      "source": [
        "# Split the data into training set and testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, stratify=y, random_state=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouOQLc3gHx0z"
      },
      "outputs": [],
      "source": [
        "# Construct a logistic regression model and fit it to the training dataset\n",
        "log_clf = LogisticRegression(random_state=100, max_iter=500).fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umJ8VW15IP8t"
      },
      "outputs": [],
      "source": [
        "# Use the logistic regression model to get predictions on the test set\n",
        "y_pred = log_clf.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7K0z0hnIeD7"
      },
      "outputs": [],
      "source": [
        "#Create a confusion matrix to visualize the results of the logistic regression model.\n",
        "\n",
        "# Compute values for confusion matrix\n",
        "log_cm = confusion_matrix(y_test, y_pred, labels=log_clf.classes_)\n",
        "\n",
        "# Create display of confusion matrix\n",
        "log_disp = ConfusionMatrixDisplay(confusion_matrix=log_cm, display_labels=log_clf.classes_)\n",
        "\n",
        "\n",
        "#plot confusion matrix\n",
        "log_disp.plot(values_format='')\n",
        "\n",
        "#display plot\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96iRKIfzHXpm"
      },
      "source": [
        "The upper-left quadrant displays the number of true negatives. The upper-right quadrant displays the number of false positives. The bottom-left quadrant displays the number of false negatives. The bottom-right quadrant displays the number of true positives.\n",
        "\n",
        "True negatives: The number of people who did not leave that the model accurately predicted did not leave.\n",
        "\n",
        "False positives: The number of people who did not leave the model inaccurately predicted as leaving.\n",
        "\n",
        "False negatives: The number of people who left that the model inaccurately predicted did not leave\n",
        "\n",
        "True positives: The number of people who left the model accurately predicted as leaving\n",
        "\n",
        "A perfect model would yield all true negatives and true positives, and no false negatives or false positives.\n",
        "\n",
        "Create a classification report that includes precision, recall, f1-score, and accuracy metrics to evaluate the performance of the logistic regression model.\n",
        "\n",
        "Check the class balance in the data. In other words, check the value counts in the left column. Since this is a binary classification task, the class balance informs the way you interpret accuracy metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LH1WmhSGJdFU"
      },
      "outputs": [],
      "source": [
        "df_logreg['left'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ86fXJwIHUW"
      },
      "source": [
        "There is an approximately 83%-17% split. So the data is not perfectly balanced, but it is not too imbalanced. If it was more severely imbalanced, you might want to resample the data to make it more balanced. In this case, you can use this data without modifying the class balance and continue evaluating the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYWL_HnNHhNq"
      },
      "outputs": [],
      "source": [
        "target_names = ['predicted would not leave', 'predicted would leave']\n",
        "print(classification_report(y_test, y_pred, target_names=target_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dq3ovnoBJWtk"
      },
      "source": [
        "The classification report above shows that the logistic regression model achieved a precision of 79%, recall of 82%, f1-score of 80% (all weighted averages), and accuracy of 82%. However, if it's most important to predict employees who leave, then the scores are significantly lower.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPUkmmXzJrMH"
      },
      "source": [
        "#MODELLING APPROACH B : TREE-BASED MODEL\n",
        "This approach covers implementation of Decision Tree and Random Forest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Tjpw1qkI0B6"
      },
      "outputs": [],
      "source": [
        "#Isolate the outcome\n",
        "y = df_en['left']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b9wna4gKMa_"
      },
      "outputs": [],
      "source": [
        "X = df_en.drop('left', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6PkojaDKUHR"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, stratify=y, random_state=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY8RcHr0K33M"
      },
      "source": [
        "**Desison tree - Round 1**\n",
        "\n",
        "Construct a decision tree model and set up cross-validated grid-search to exhuastively search for the best model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-AqCGVQQKy2u"
      },
      "outputs": [],
      "source": [
        "#instantiate model\n",
        "tree = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "#assign a dictionary of hyperparameters to search over\n",
        "cv_params = {'max_depth':[4,6,8,None], 'min_samples_leaf': [2,5,1], 'min_samples_split':[2,4,6]}\n",
        "\n",
        "#assign a dic of scoring metrics to capture\n",
        "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
        "\n",
        "#instantiate GridSearch\n",
        "tree1 = GridSearchCV(tree, cv_params, scoring=scoring, cv=4, refit='roc_auc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUIV6hoLMQGN"
      },
      "source": [
        "Fit the decision tree model to the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2Fz8xlJMGnd"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "tree1.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7_YaqBiMpjC"
      },
      "source": [
        "Identify the optimal values for the decision tree parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUtU4e5pMX5i"
      },
      "outputs": [],
      "source": [
        "tree1.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOKMhl8-Mrup"
      },
      "source": [
        "Identify the best AUC score achieved by the decision tree model on the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWV62L_KMmha"
      },
      "outputs": [],
      "source": [
        "tree1.best_score_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzTDa2qjM0_z"
      },
      "source": [
        "This is a strong AUC score, which shows that this model can predict employees who will leave very well.\n",
        "\n",
        "Next, you can write a function that will help you extract all the scores from the grid search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwJfZm0XMw3V"
      },
      "outputs": [],
      "source": [
        "def make_results(model_name:str, model_object, metric:str):\n",
        "    '''\n",
        "    Arguments:\n",
        "        model_name (string): what you want the model to be called in the output table\n",
        "        model_object: a fit GridSearchCV object\n",
        "        metric (string): precision, recall, f1, accuracy, or auc\n",
        "\n",
        "    Returns a pandas df with the F1, recall, precision, accuracy, and auc scores\n",
        "    for the model with the best mean 'metric' score across all validation folds.\n",
        "    '''\n",
        "\n",
        "    # Create dictionary that maps input metric to actual metric name in GridSearchCV\n",
        "    metric_dict = {'auc': 'mean_test_roc_auc',\n",
        "                   'precision': 'mean_test_precision',\n",
        "                   'recall': 'mean_test_recall',\n",
        "                   'f1': 'mean_test_f1',\n",
        "                   'accuracy': 'mean_test_accuracy'\n",
        "                  }\n",
        "\n",
        "    # Get all the results from the CV and put them in a df\n",
        "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
        "\n",
        "    # Isolate the row of the df with the max(metric) score\n",
        "    best_estimator_results = cv_results.iloc[cv_results[metric_dict[metric]].idxmax(), :]\n",
        "\n",
        "    # Extract Accuracy, precision, recall, and f1 score from that row\n",
        "    auc = best_estimator_results.mean_test_roc_auc\n",
        "    f1 = best_estimator_results.mean_test_f1\n",
        "    recall = best_estimator_results.mean_test_recall\n",
        "    precision = best_estimator_results.mean_test_precision\n",
        "    accuracy = best_estimator_results.mean_test_accuracy\n",
        "\n",
        "    # Create table of results\n",
        "    table = pd.DataFrame()\n",
        "    table = pd.DataFrame({'model': [model_name],\n",
        "                          'precision': [precision],\n",
        "                          'recall': [recall],\n",
        "                          'F1': [f1],\n",
        "                          'accuracy': [accuracy],\n",
        "                          'auc': [auc]\n",
        "                        })\n",
        "\n",
        "    return table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFj4MM2TQSNQ"
      },
      "source": [
        "Use the function just defined to get all the scores from grid search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTbvSJBPNiI6"
      },
      "outputs": [],
      "source": [
        "# Get all CV scores\n",
        "tree1_cv_results = make_results('decision tree cv', tree1, 'auc')\n",
        "tree1_cv_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhwV4Kc7QZ-k"
      },
      "source": [
        "All of these scores from the decision tree model are strong indicators of good model performance.\n",
        "\n",
        "Recall that decision trees can be vulnerable to overfitting, and random forests avoid overfitting by incorporating multiple trees to make predictions. You could construct a random forest model next."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lj7wcsjVQWBD"
      },
      "outputs": [],
      "source": [
        "# Instantiate model\n",
        "rf = RandomForestClassifier(random_state=0)\n",
        "\n",
        "# Assign a dictionary of hyperparameters to search over\n",
        "cv_params = {'max_depth': [3,5, None],\n",
        "             'max_features': [1.0],\n",
        "             'max_samples': [0.7, 1.0],\n",
        "             'min_samples_leaf': [1,2,3],\n",
        "             'min_samples_split': [2,3,4],\n",
        "             'n_estimators': [300, 500],\n",
        "             }\n",
        "\n",
        "# Assign a dictionary of scoring metrics to capture\n",
        "scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}\n",
        "\n",
        "# Instantiate GridSearch\n",
        "rf1 = GridSearchCV(rf, cv_params, scoring=scoring, cv=4, refit='roc_auc')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbzYloq-QnZz"
      },
      "source": [
        "Fit the random forest model to the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KeIO5_KnQleu"
      },
      "outputs": [],
      "source": [
        "#%%time\n",
        "rf1.fit(X_train, y_train) # --> Wall time: ~10min"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZTzTTGLQ3dF"
      },
      "source": [
        "Specify path to where you want to save your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cO3zsWRhQqNC"
      },
      "outputs": [],
      "source": [
        "# Define a path to the folder where want to save the model\n",
        "path = 'C:\\DATASET\\Dataset from google in Coursera\\model HR'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iMIBzgZRPNA"
      },
      "source": [
        "Define functions to pickle the model and read in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Q8FQaRMdRMFz"
      },
      "outputs": [],
      "source": [
        "def write_pickle(path, model_object, save_as:str):\n",
        "    '''\n",
        "    In:\n",
        "        path:         path of folder where you want to save the pickle\n",
        "        model_object: a model you want to pickle\n",
        "        save_as:      filename for how you want to save the model\n",
        "\n",
        "    Out: A call to pickle the model in the folder indicated\n",
        "    '''\n",
        "\n",
        "    with open(path + save_as + '.pickle', 'wb') as to_write:\n",
        "        pickle.dump(model_object, to_write)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rA_50Bqqfaln"
      },
      "outputs": [],
      "source": [
        "def read_pickle(path, saved_model_name:str):\n",
        "    '''\n",
        "    In:\n",
        "        path:             path to folder where you want to read from\n",
        "        saved_model_name: filename of pickled model you want to read in\n",
        "\n",
        "    Out:\n",
        "        model: the pickled model\n",
        "    '''\n",
        "    with open(path + saved_model_name + '.pickle', 'rb') as to_read:\n",
        "        model = pickle.load(to_read)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzerKSP0fgBm"
      },
      "source": [
        "Use the functions defined above to save the model in a pickle file and then read it in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B3XBiiOdfdLa"
      },
      "outputs": [],
      "source": [
        "# Write pickle\n",
        "write_pickle(path, rf1, 'hr_rf1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ErgEMPgUfkHd"
      },
      "outputs": [],
      "source": [
        "# Read pickle\n",
        "rf1 = read_pickle(path, 'hr_rf1')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "besJMY1ufqLr"
      },
      "source": [
        "Identify the best AUC score achieved by the random forest model on the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jniF9MFZfnhc"
      },
      "outputs": [],
      "source": [
        "# Check best AUC score on CV\n",
        "rf1.best_score_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJTtpQwcft4E"
      },
      "source": [
        "Identify the optimal values for the parameters of the random forest model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FOmrZzDpftQN"
      },
      "outputs": [],
      "source": [
        "# Check best params\n",
        "rf1.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3Y0TbbvsfzBm"
      },
      "outputs": [],
      "source": [
        "# Check best params\n",
        "rf1.best_params_\n",
        "# Check best params\n",
        "rf1.best_params_\n",
        "{'max_depth': 5,\n",
        " 'max_features': 1.0,\n",
        " 'max_samples': 0.7,\n",
        " 'min_samples_leaf': 1,\n",
        " 'min_samples_split': 4,\n",
        " 'n_estimators': 500}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6stKoAZkf4Bo"
      },
      "source": [
        "Collect the evaluation scores on the training set for the decision tree and random forest models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7OCYpwkdf15M"
      },
      "outputs": [],
      "source": [
        "# Get all CV scores\n",
        "rf1_cv_results = make_results('random forest cv', rf1, 'auc')\n",
        "print(tree1_cv_results)\n",
        "print(rf1_cv_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkhFsqkhf-fi"
      },
      "source": [
        "The evaluation scores of the random forest model are better than those of the decision tree model, with the exception of recall (the recall score of the random forest model is approximately 0.001 lower, which is a negligible amount). This indicates that the random forest model mostly outperforms the decision tree model.\n",
        "\n",
        "Next, you can evaluate the final model on the test set.\n",
        "\n",
        "Define a function that gets all the scores from a model's predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IstwqvBHf6f4"
      },
      "outputs": [],
      "source": [
        "def get_scores(model_name:str, model, X_test_data, y_test_data):\n",
        "    '''\n",
        "    Generate a table of test scores.\n",
        "\n",
        "    In:\n",
        "        model_name (string):  How you want your model to be named in the output table\n",
        "        model:                A fit GridSearchCV object\n",
        "        X_test_data:          numpy array of X_test data\n",
        "        y_test_data:          numpy array of y_test data\n",
        "\n",
        "    Out: pandas df of precision, recall, f1, accuracy, and AUC scores for your model\n",
        "    '''\n",
        "\n",
        "    preds = model.best_estimator_.predict(X_test_data)\n",
        "\n",
        "    auc = roc_auc_score(y_test_data, preds)\n",
        "    accuracy = accuracy_score(y_test_data, preds)\n",
        "    precision = precision_score(y_test_data, preds)\n",
        "    recall = recall_score(y_test_data, preds)\n",
        "    f1 = f1_score(y_test_data, preds)\n",
        "\n",
        "    table = pd.DataFrame({'model': [model_name],\n",
        "                          'precision': [precision],\n",
        "                          'recall': [recall],\n",
        "                          'f1': [f1],\n",
        "                          'accuracy': [accuracy],\n",
        "                          'AUC': [auc]\n",
        "                         })\n",
        "\n",
        "    return table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I0n-WgIgEWa"
      },
      "source": [
        "Now use the best performing model to predict on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EYNDlz54gBHU"
      },
      "outputs": [],
      "source": [
        "# Get predictions on test data\n",
        "rf1_test_scores = get_scores('random forest1 test', rf1, X_test, y_test)\n",
        "rf1_test_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58jwIGklgLH7"
      },
      "source": [
        "The test scores are very similar to the validation scores, which is good. This appears to be a strong model. Since this test set was only used for this model, you can be more confident that your model's performance on this data is representative of how it will perform on new, unseeen data.\n",
        "\n",
        "Feature Engineering\n",
        "You might be skeptical of the high evaluation scores. There is a chance that there is some data leakage occurring. Data leakage is when you use data to train your model that should not be used during training, either because it appears in the test data or because it's not data that you'd expect to have when the model is actually deployed. Training a model with leaked data can give an unrealistic score that is not replicated in production.\n",
        "\n",
        "In this case, it's likely that the company won't have satisfaction levels reported for all of its employees. It's also possible that the average_monthly_hours column is a source of some data leakage. If employees have already decided upon quitting, or have already been identified by management as people to be fired, they may be working fewer hours.\n",
        "\n",
        "The first round of decision tree and random forest models included all variables as features. This next round will incorporate feature engineering to build improved models.\n",
        "\n",
        "You could proceed by dropping satisfaction_level and creating a new feature that roughly captures whether an employee is overworked. You could call this new feature overworked. It will be a binary variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yZ4SQrzBgH--"
      },
      "outputs": [],
      "source": [
        "# Drop `satisfaction_level` and save resulting dataframe in new variable\n",
        "df2 = df_en.drop('satisfaction_level', axis=1)\n",
        "\n",
        "# Display first few rows of new dataframe\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "muOirbPMgN8d"
      },
      "outputs": [],
      "source": [
        "# Create `overworked` column. For now, it's identical to average monthly hours.\n",
        "df2['overworked'] = df2['average_monthly_hours']\n",
        "\n",
        "# Inspect max and min average monthly hours values\n",
        "print('Max hours:', df2['overworked'].max())\n",
        "print('Min hours:', df2['overworked'].min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpgBUO3JgYGY"
      },
      "source": [
        "166.67 is approximately the average number of monthly hours for someone who works 50 weeks per year, 5 days per week, 8 hours per day.\n",
        "\n",
        "You could define being overworked as working more than 175 hours per month on average.\n",
        "\n",
        "To make the overworked column binary, you could reassign the column using a boolean mask.\n",
        "\n",
        "df3['overworked'] > 175 creates a series of booleans, consisting of True for every value > 175 and False for every values â‰¤ 175\n",
        ".astype(int) converts all True to 1 and all False to 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zx2-BtlsgUjH"
      },
      "outputs": [],
      "source": [
        "# Define `overworked` as working > 175 hrs/week\n",
        "df2['overworked'] = (df2['overworked'] > 175).astype(int)\n",
        "\n",
        "# Display first few rows of new column\n",
        "df2['overworked'].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "954_kRQAgbM_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPuoyzwVJwu2zLm/bnG89I",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}